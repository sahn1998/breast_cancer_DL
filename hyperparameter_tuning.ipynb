{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global packages\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_curve, roc_auc_score, auc\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.3322, 0.0275, 0.1132],\n",
    "    std=[0.2215, 0.0965, 0.3152],\n",
    ")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(\"./Dataset_BUSI/Dataset_BUSI_with_GT/train/\", transform=transform)\n",
    "\n",
    "# Get the labels from the dataset\n",
    "labels = np.array(dataset.targets)\n",
    "\n",
    "# Split the dataset into train and test sets while maintaining class proportions\n",
    "train_indices, test_indices = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Create Subset datasets for train and test\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Optimizer\n",
    "Currently the optimizer is an **Adam Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model,\n",
    "                    learning_rate_pretrained,\n",
    "                    learning_rate_new,\n",
    "                    weight_decay,\n",
    "                    beta1=0.9,  # Adam parameter\n",
    "                    beta2=0.999,  # Adam parameter\n",
    "                    eps=1e-8,  # Adam parameter\n",
    "                    amsgrad=False):  # Adam parameter\n",
    "    params_pretrained = []\n",
    "    params_new = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier.6' in name:\n",
    "            params_new.append(param)\n",
    "        else:\n",
    "            params_pretrained.append(param)\n",
    "\n",
    "    '''Setting Loss Functions and Optimizers'''\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': params_pretrained, 'lr': learning_rate_pretrained},\n",
    "        {'params': params_new, 'lr': learning_rate_new},\n",
    "    ], weight_decay=weight_decay, betas=(beta1, beta2), eps=eps, amsgrad=amsgrad)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function\n",
    "Training function for the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, device, train_loader, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Training function for a neural network model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to be trained.\n",
    "        device (torch.device): The device (e.g., 'cuda' or 'cpu') to use for training.\n",
    "        train_loader (DataLoader): DataLoader containing training data.\n",
    "        criterion: The loss function.\n",
    "        optimizer: The optimization algorithm.\n",
    "\n",
    "    Returns:\n",
    "        float: The average training loss for one epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move the model to the specified device\n",
    "    model.to(device)\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize variables to keep track of training loss and total samples\n",
    "    train_loss, total_samples = 0.0, 0\n",
    "\n",
    "    # Loop through the batches in the training data\n",
    "    for images, labels in train_loader:\n",
    "        # Move data to the specified device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients of the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(images)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "\n",
    "        # Backpropagation: Compute gradients\n",
    "        loss.backward()\n",
    "        # Update the model's parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the training loss for the current batch\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        # Count the total number of processed samples\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate the average training loss for the entire epoch\n",
    "    avg_train_loss = train_loss / total_samples\n",
    "\n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Function (Evaluation)\n",
    "Evaluation function for the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, device, val_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluation function for a neural network model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to be evaluated.\n",
    "        device (torch.device): The device (e.g., 'cuda' or 'cpu') to use for evaluation.\n",
    "        val_loader (DataLoader): DataLoader containing validation data.\n",
    "        criterion: The loss function.\n",
    "\n",
    "    Returns:\n",
    "        float: The average validation loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move the model to the specified device\n",
    "    model.to(device)\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to keep track of validation loss and total samples\n",
    "    val_loss, total_samples = 0.0, 0\n",
    "\n",
    "    # Disable gradient calculation during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Loop through the batches in the validation data\n",
    "        for images, labels in val_loader:\n",
    "            # Move data to the specified device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Forward pass through the model\n",
    "            outputs = model(images)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "\n",
    "            # Accumulate the validation loss for the current batch\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            # Count the total number of processed samples\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate the average validation loss\n",
    "    avg_val_loss = val_loss / total_samples\n",
    "\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Validation Loaders\n",
    "Creates data loaders for training and validation using given indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_loader(train_dataset, train_idx, val_idx, batch_size):\n",
    "    \"\"\"\n",
    "    Creates data loaders for training and validation using given indices.\n",
    "\n",
    "    Args:\n",
    "        train_dataset (Dataset): The training dataset.\n",
    "        train_idx (list): List of indices for the training set.\n",
    "        val_idx (list): List of indices for the validation set.\n",
    "        batch_size (int): Batch size for the data loaders.\n",
    "\n",
    "    Returns:\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        val_loader (DataLoader): DataLoader for the validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create subset samplers for training and validation indices\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    # Check for overlapping indices between train and validation sets\n",
    "    assert len(set(train_idx).intersection(val_idx)) == 0, \"Indices overlap between train and validation sets.\"\n",
    "\n",
    "    # Define data loaders for training and validation data in this fold\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_subsampler,\n",
    "        shuffle=True  # Shuffle the training data for each epoch\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=val_subsampler\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Search Space\n",
    "Define hyperparameters and search space for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters and search space for hyperparameter tuning\n",
    "param_search_space = {\n",
    "    'num_frozen_layers': (4, 7),\n",
    "    'learning_rate_pretrained': (0.00001, 0.001),\n",
    "    'learning_rate_new': (0.00001, 0.001),\n",
    "    'weight_decay': (0.005, 0.01)\n",
    "}\n",
    "\n",
    "# Number of random configurations to try during hyperparameter search\n",
    "num_random_configs = 20\n",
    "\n",
    "# Set training parameters\n",
    "num_epochs = 20  # Number of training epochs\n",
    "batch_size = 32  # Batch size for training\n",
    "k = 5  # Number of folds for cross-validation\n",
    "\n",
    "# Create a dictionary to store fold results\n",
    "fold_results = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "# Create a StratifiedKFold cross-validator with specified number of folds\n",
    "skfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the loss function for the model\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy Loss with Sigmoid Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "The code iterates through cross-validation folds, tests various hyperparameter settings, and records the best configuration with the lowest average validation loss for each fold. <br>\n",
    "\n",
    "I am trying to get through as many hyperparameter settings as I can, so I am not doing CVs for each randomized hyperparameter but rather looking at random hyperparameter settings for each fold. This allows me to go through a large number of hyperparameters while saving computational time and resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store best configurations and average validation losses per fold\n",
    "best_config_per_fold = []\n",
    "\n",
    "# Loop through the folds using StratifiedKFold\n",
    "for fold, (train_idx, val_idx) in enumerate(skfold.split(train_dataset, labels[train_indices])):\n",
    "\n",
    "    # Create data loaders for training and validation using the current fold indices\n",
    "    train_loader, val_loader = create_train_val_loader(train_dataset, train_idx, val_idx, batch_size)\n",
    "\n",
    "    # Create a list to store configurations and their corresponding average validation losses for this fold\n",
    "    fold_configs = []\n",
    "\n",
    "    # Try different random configurations for hyperparameters\n",
    "    for _ in range(num_random_configs):\n",
    "        # Initialize the best validation loss for the current configuration\n",
    "        epoch_best_val_loss = float('inf')\n",
    "\n",
    "        # Initialize a VGG16 model pre-trained on ImageNet\n",
    "        model = models.vgg16(pretrained=True)\n",
    "\n",
    "        # Randomly sample hyperparameters from the specified intervals\n",
    "        random_config = {\n",
    "            param: random.uniform(min_val, max_val) for param, (min_val, max_val) in param_search_space.items()\n",
    "        }\n",
    "\n",
    "        # Extract hyperparameters from the random configuration\n",
    "        num_frozen_layers = int(random_config['num_frozen_layers'])\n",
    "        learning_rate_pretrained = random_config['learning_rate_pretrained']\n",
    "        learning_rate_new = random_config['learning_rate_new']\n",
    "        weight_decay = random_config['weight_decay']\n",
    "\n",
    "        # Create an optimizer with the sampled hyperparameters\n",
    "        optimizer = create_optimizer(model,\n",
    "                                     learning_rate_pretrained=learning_rate_pretrained,\n",
    "                                     learning_rate_new=learning_rate_new,\n",
    "                                     weight_decay=weight_decay)\n",
    "\n",
    "        # Print the current hyperparameter configuration\n",
    "        print(\"Num Frozen Layers:\", num_frozen_layers)\n",
    "        print(\"Learning Rate Pretrained:\", \"{:.8f}\".format(learning_rate_pretrained))\n",
    "        print(\"Learning Rate New:\", \"{:.8f}\".format(learning_rate_new))\n",
    "        print(\"Weight Decay:\", \"{:.8f}\".format(weight_decay))\n",
    "\n",
    "        # Replace the last fully connected layer to match the binary classification task\n",
    "        in_features = model.classifier[6].in_features\n",
    "        model.classifier[6] = nn.Linear(in_features=4096, out_features=1)\n",
    "\n",
    "        # Freeze some layers based on the specified number\n",
    "        for layer in model.features[:num_frozen_layers]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Lists to store current configuration's average training and validation losses\n",
    "        current_config_avg_train_losses = []\n",
    "        current_config_avg_val_losses = []\n",
    "\n",
    "        # Train and validate the model for the specified number of epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            avg_train_loss = training(model, device, train_loader, criterion, optimizer)\n",
    "            avg_val_loss = testing(model, device, val_loader, criterion)\n",
    "\n",
    "            print(\"Epoch:{}/{} AVG Training Loss:{:.6f} AVG Val Loss:{:.6f}\".format(epoch + 1, num_epochs, avg_train_loss, avg_val_loss))\n",
    "\n",
    "            # Update best validation loss and save model weights if necessary\n",
    "            if avg_val_loss < epoch_best_val_loss:\n",
    "                epoch_best_val_loss = avg_val_loss\n",
    "\n",
    "            # Store current epoch's average training and validation losses\n",
    "            current_config_avg_train_losses.append(avg_train_loss)\n",
    "            current_config_avg_val_losses.append(avg_val_loss)\n",
    "\n",
    "        # Store the configuration and its average validation loss for this fold\n",
    "        fold_configs.append((random_config, np.mean(current_config_avg_val_losses)))\n",
    "\n",
    "    # Find the best configuration with the lowest average validation loss for this fold\n",
    "    fold_best_val_loss = float('inf')\n",
    "    fold_best_config = None\n",
    "    for config, avg_val_loss in fold_configs:\n",
    "        if avg_val_loss < fold_best_val_loss:\n",
    "            fold_best_val_loss = avg_val_loss\n",
    "            fold_best_config = config\n",
    "\n",
    "    # Store the best configuration and its associated average validation loss for this fold\n",
    "    best_config_per_fold.append((fold_best_config, fold_best_val_loss))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
