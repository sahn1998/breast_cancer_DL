{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global packages\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_curve, roc_auc_score, auc\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.3322, 0.0275, 0.1132],\n",
    "    std=[0.2215, 0.0965, 0.3152],\n",
    ")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(\"./Dataset_BUSI/Dataset_BUSI_with_GT/train/\", transform=transform)\n",
    "\n",
    "# Get the labels from the dataset\n",
    "labels = np.array(dataset.targets)\n",
    "\n",
    "# Split the dataset into train and test sets while maintaining class proportions\n",
    "train_indices, test_indices = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Create Subset datasets for train and test\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure train and test indices don't overlap\n",
    "assert len(set(train_indices).intersection(test_indices)) == 0, \"Indices overlap between train and validation sets.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the distribution\n",
    "This is to make sure that the distributions are equal across the Original, Train, and Test datasets before training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_distribution(header, distribution):\n",
    "    \"\"\"\n",
    "    Prints the class distribution of a dataset.\n",
    "\n",
    "    Args:\n",
    "        header (str): Header text to indicate the dataset type.\n",
    "        distribution (dict): A dictionary containing class labels and their ratios.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"{header} Dataset Ratio:\")\n",
    "    for label, ratio in distribution.items():\n",
    "        class_name = \"Benign\" if label == 0 else \"Malignant\"\n",
    "        print(f\"{class_name}: {ratio:.2%} of the {header.lower()} dataset\")\n",
    "\n",
    "# Count the occurrences of each label in the dataset\n",
    "label_counts = Counter(dataset.targets)\n",
    "total_samples = len(dataset)\n",
    "labels = np.array(dataset.targets)\n",
    "\n",
    "# Filter indices for the train and test sets\n",
    "train_indices = [idx for idx in range(len(dataset)) if idx in train_indices]\n",
    "test_indices = [idx for idx in range(len(dataset)) if idx in test_indices]\n",
    "\n",
    "def calculate_distribution(indices):\n",
    "    \"\"\"\n",
    "    Calculates the class distribution for a given set of indices.\n",
    "\n",
    "    Args:\n",
    "        indices (list): List of indices for a specific dataset subset.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing class labels and their ratios in the subset.\n",
    "    \"\"\"\n",
    "    total_samples = len(indices)\n",
    "    return {class_label: np.sum(labels[indices] == class_label) / total_samples for class_label in np.unique(labels)}\n",
    "\n",
    "# Calculate class distribution for the train and test sets\n",
    "train_class_distribution = calculate_distribution(train_indices)\n",
    "test_class_distribution = calculate_distribution(test_indices)\n",
    "\n",
    "# Print the class distribution of the main dataset\n",
    "print(\"Main Dataset Ratio:\")\n",
    "for label, count in label_counts.items():\n",
    "    label_str = dataset.classes[label]\n",
    "    ratio = count / total_samples\n",
    "    print(f\"{label_str}: {count} samples ({ratio:.2%} of the main dataset)\")\n",
    "\n",
    "# Print class distribution for train and test sets\n",
    "print_distribution(\"Train\", train_class_distribution)\n",
    "print_distribution(\"Test\", test_class_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Optimizer\n",
    "Currently the optimizer is an **Adam Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model,  # Function that creates an optimizer for a given model and hyperparameters\n",
    "                    learning_rate_pretrained=0.00039710,  # Learning rate for pretrained layers\n",
    "                    learning_rate_new=0.00083336,  # Learning rate for new layers\n",
    "                    weight_decay=0.0073691,  # Weight decay (L2 regularization) coefficient\n",
    "                    beta1=0.9,  # Exponential decay rate for the first moment estimates (Adam parameter)\n",
    "                    beta2=0.999,  # Exponential decay rate for the second moment estimates (Adam parameter)\n",
    "                    eps=1e-8,  # Small constant to prevent division by zero (Adam parameter)\n",
    "                    amsgrad=False):  # Flag for using the AMSGrad variant of Adam optimizer\n",
    "\n",
    "    # Separate model parameters into two groups: pretrained and new\n",
    "    params_pretrained = []  # Parameters from the pretrained layers\n",
    "    params_new = []         # Parameters from the new layers\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier.6' in name:\n",
    "            params_new.append(param)  # Parameters from the new layers\n",
    "        else:\n",
    "            params_pretrained.append(param)  # Parameters from the pretrained layers\n",
    "\n",
    "    '''Setting Loss Functions and Optimizers'''\n",
    "    # Create an Adam optimizer with different learning rates for the two parameter groups\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': params_pretrained, 'lr': learning_rate_pretrained},  # Pretrained layer parameters with specific learning rate\n",
    "        {'params': params_new, 'lr': learning_rate_new},                # New layer parameters with different learning rate\n",
    "    ], weight_decay=weight_decay, betas=(beta1, beta2), eps=eps, amsgrad=amsgrad)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function\n",
    "Training function for the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, device, train_loader, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Training function for a neural network model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to be trained.\n",
    "        device (torch.device): The device (e.g., 'cuda' or 'cpu') to use for training.\n",
    "        train_loader (DataLoader): DataLoader containing training data.\n",
    "        criterion: The loss function.\n",
    "        optimizer: The optimization algorithm.\n",
    "\n",
    "    Returns:\n",
    "        float: The average training loss for one epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move the model to the specified device\n",
    "    model.to(device)\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize variables to keep track of training loss and total samples\n",
    "    train_loss, total_samples = 0.0, 0\n",
    "\n",
    "    # Loop through the batches in the training data\n",
    "    for images, labels in train_loader:\n",
    "        # Move data to the specified device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients of the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(images)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "\n",
    "        # Backpropagation: Compute gradients\n",
    "        loss.backward()\n",
    "        # Update the model's parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate the training loss for the current batch\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        # Count the total number of processed samples\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate the average training loss for the entire epoch\n",
    "    avg_train_loss = train_loss / total_samples\n",
    "\n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Function (Evaluation)\n",
    "\n",
    "Evaluation function for the VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, device, val_loader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluation function for a neural network model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to be evaluated.\n",
    "        device (torch.device): The device (e.g., 'cuda' or 'cpu') to use for evaluation.\n",
    "        val_loader (DataLoader): DataLoader containing validation data.\n",
    "        criterion: The loss function.\n",
    "\n",
    "    Returns:\n",
    "        float: The average validation loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Move the model to the specified device\n",
    "    model.to(device)\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize variables to keep track of validation loss and total samples\n",
    "    val_loss, total_samples = 0.0, 0\n",
    "\n",
    "    # Disable gradient calculation during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Loop through the batches in the validation data\n",
    "        for images, labels in val_loader:\n",
    "            # Move data to the specified device\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # Forward pass through the model\n",
    "            outputs = model(images)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "\n",
    "            # Accumulate the validation loss for the current batch\n",
    "            val_loss += loss.item() * labels.size(0)\n",
    "            # Count the total number of processed samples\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate the average validation loss\n",
    "    avg_val_loss = val_loss / total_samples\n",
    "\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train and Validation Loaders\n",
    "Creates data loaders for training and validation using given indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_loader(train_dataset, train_idx, val_idx, batch_size):\n",
    "    \"\"\"\n",
    "    Creates data loaders for training and validation using given indices.\n",
    "\n",
    "    Args:\n",
    "        train_dataset (Dataset): The training dataset.\n",
    "        train_idx (list): List of indices for the training set.\n",
    "        val_idx (list): List of indices for the validation set.\n",
    "        batch_size (int): Batch size for the data loaders.\n",
    "\n",
    "    Returns:\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        val_loader (DataLoader): DataLoader for the validation data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create subset samplers for training and validation indices\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "    # Check for overlapping indices between train and validation sets\n",
    "    assert len(set(train_idx).intersection(val_idx)) == 0, \"Indices overlap between train and validation sets.\"\n",
    "\n",
    "    # Define data loaders for training and validation data in this fold\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_subsampler,\n",
    "        shuffle=True  # Shuffle the training data for each epoch\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=val_subsampler\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Setting training parameters'''\n",
    "num_frozen_layers = 5\n",
    "num_epochs = 20 # epochs\n",
    "batch_size = 32 # batch size\n",
    "k=5 # fold number\n",
    "\n",
    "'''Setting cross-validation parameters'''\n",
    "fold_results={'train_loss': [],\n",
    "              'val_loss': []} # dictionary to memorize all the scores\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42) # sklearn library\n",
    "\n",
    "'''Setting Loss Function'''\n",
    "criterion = nn.BCEWithLogitsLoss() # Binary Cross Entropy Loss with Sigmoid Function (only have to classify 2 things -> 0 or 1): With multiple classes you generally want to use softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking distributions of each fold\n",
    "Validates the class distribution and indices of a cross-validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_distribution(k, fold, train_indices, train_idx, val_idx):\n",
    "    \"\"\"\n",
    "    Validates the class distribution and indices of a cross-validation fold.\n",
    "\n",
    "    Args:\n",
    "        k (int): Total number of folds in cross-validation.\n",
    "        fold (int): Current fold number.\n",
    "        train_indices (list): List of indices for the entire dataset.\n",
    "        train_idx (list): List of indices for the training set in the current fold.\n",
    "        val_idx (list): List of indices for the validation set in the current fold.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate class distribution for training and validation sets\n",
    "    train_class_distribution = {class_label: np.sum(labels[train_indices][train_idx] == class_label) for class_label in np.unique(labels)}\n",
    "    val_class_distribution = {class_label: np.sum(labels[train_indices][val_idx] == class_label) for class_label in np.unique(labels)}\n",
    "\n",
    "    # Calculate class ratios for training and validation sets\n",
    "    total_train_samples = len(train_idx)\n",
    "    total_val_samples = len(val_idx)\n",
    "    train_class_ratios = {class_label: count / total_train_samples for class_label, count in train_class_distribution.items()}\n",
    "    val_class_ratios = {class_label: count / total_val_samples for class_label, count in val_class_distribution.items()}\n",
    "\n",
    "    # Check dataset size and indices consistency\n",
    "    total_samples = len(train_indices)\n",
    "    train_samples = len(train_idx)\n",
    "    val_samples = len(val_idx)\n",
    "    assert total_samples == train_samples + val_samples, \"Train and validation set sizes do not add up.\"\n",
    "\n",
    "    # Check for index uniqueness and non-overlapping\n",
    "    assert len(set(train_idx).intersection(val_idx)) == 0, \"Indices overlap between train and validation sets.\"\n",
    "\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Fold {fold + 1}/{k}\")\n",
    "\n",
    "    # Print class ratios for training set\n",
    "    for train_label, train_ratio in train_class_ratios.items():\n",
    "        class_name = \"Benign\" if train_label == 0 else \"Malignant\"\n",
    "        print(f\"{class_name}: {train_ratio:.2%} of the CV-TRAIN dataset\")\n",
    "\n",
    "    # Print class ratios for validation set\n",
    "    for val_label, val_ratio in val_class_ratios.items():\n",
    "        class_name = \"Benign\" if val_label == 0 else \"Malignant\"\n",
    "        print(f\"{class_name}: {val_ratio:.2%} of the CV-VALID dataset\")\n",
    "\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "best_models_per_fold = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skfold.split(train_dataset, labels[train_indices])):\n",
    "    # Check distribution of indices\n",
    "    check_distribution(k, fold, train_indices, train_idx, val_idx)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_loader, val_loader = create_train_val_loader(train_dataset, train_idx, val_idx, batch_size)\n",
    "\n",
    "    # Initiate the VGG16\n",
    "    model = models.vgg16(pretrained=True)\n",
    "\n",
    "    in_features = model.classifier[6].in_features\n",
    "    model.classifier[6] = nn.Linear(in_features=4096, out_features=1)\n",
    "\n",
    "    # Freezing some layers\n",
    "    for layer in model.features[:num_frozen_layers]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Initiating our optimizer\n",
    "    optimizer = create_optimizer(model)\n",
    "\n",
    "    # Iterative epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train and Val\n",
    "        avg_train_loss = training(model, device, train_loader, criterion, optimizer)\n",
    "        avg_val_loss = testing(model, device, val_loader, criterion)\n",
    "\n",
    "        print(\"Epoch:{}/{} AVG Training Loss:{:.6f} AVG Val Loss:{:.6f}\".format(epoch + 1, num_epochs, avg_train_loss, avg_val_loss))\n",
    "\n",
    "        # Update best validation loss and save model weights if necessary\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        # Save results\n",
    "        fold_results['train_loss'].append(avg_train_loss)\n",
    "        fold_results['val_loss'].append(avg_val_loss)\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    best_models_per_fold.append((model, best_val_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model\n",
    "Testing the model on the holdout evaluation set that we created at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset sampler for the test indices\n",
    "test_subsampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "# Create a DataLoader for the test data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=test_subsampler,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# List of threshold values for evaluation\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# List to store F1 scores for each fold\n",
    "f1_scores_per_fold = []\n",
    "\n",
    "# Extract models from the list of best models per fold\n",
    "extracted_models = [model for model, _ in best_models_per_fold]\n",
    "\n",
    "# Loop through extracted models for evaluation\n",
    "for model in extracted_models:\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predicted_labels_per_threshold = {threshold: [] for threshold in thresholds}\n",
    "\n",
    "    # Disable gradient calculation during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Loop through test data batches\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Apply sigmoid activation to model outputs\n",
    "            predicted_probs = torch.sigmoid(outputs)\n",
    "            \n",
    "            # Iterate through thresholds and make predictions\n",
    "            for threshold in all_predicted_labels_per_threshold:\n",
    "                predicted_labels = (predicted_probs >= threshold).float()  # Apply threshold\n",
    "                all_predicted_labels_per_threshold[threshold].extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    f1_scores = []\n",
    "\n",
    "    # Calculate F1 scores for different thresholds\n",
    "    for threshold, predicted_labels in all_predicted_labels_per_threshold.items():\n",
    "        f1_scores.append(f1_score(all_labels, predicted_labels, average='micro'))\n",
    "\n",
    "    f1_scores_per_fold.append(f1_scores)\n",
    "\n",
    "# Find the highest F1 score and corresponding model and threshold\n",
    "max_f1 = -1\n",
    "best_model_idx = -1\n",
    "best_threshold_idx = -1\n",
    "\n",
    "for model_idx, model in enumerate(extracted_models):\n",
    "    for threshold_idx, f1_score_value in enumerate(f1_scores_per_fold[model_idx]):\n",
    "        if f1_score_value > max_f1:\n",
    "            max_f1 = f1_score_value\n",
    "            best_model_idx = model_idx\n",
    "            best_threshold_idx = threshold_idx\n",
    "\n",
    "# Get the best model and corresponding threshold\n",
    "best_model = extracted_models[best_model_idx]\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "best_predicted_labels = all_predicted_labels_per_threshold[best_threshold]\n",
    "\n",
    "# Calculate precision and recall at the best point\n",
    "best_precision = precision_score(all_labels, best_predicted_labels, average='micro')\n",
    "best_recall = recall_score(all_labels, best_predicted_labels, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of the tuple with the lowest validation loss in the list\n",
    "index_of_lowest_loss = min(range(len(best_models_per_fold)), key=lambda i: best_models_per_fold[i][1])\n",
    "\n",
    "# Retrieve the best model and its associated lowest validation loss using the found index\n",
    "best_model_with_lowest_loss, lowest_loss = best_models_per_fold[index_of_lowest_loss]\n",
    "\n",
    "print(\"Index of the tuple with the lowest best_val_loss:\", index_of_lowest_loss)\n",
    "print(\"Lowest loss:\", lowest_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results\n",
    "print(f\"Highest F1 Score: {max_f1}\")\n",
    "print(f\"Best Model Index: {best_model_idx}\")\n",
    "print(f\"Best Threshold: {best_threshold}\")\n",
    "print(f\"Precision Score at Best Point: {best_precision}\")\n",
    "print(f\"Recall Score at Best Point: {best_recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphing\n",
    "fold_idx = num_epochs * best_model_idx\n",
    "best_train_loss = fold_results['train_loss'][fold_idx : fold_idx+num_epochs]\n",
    "best_test_loss = fold_results['val_loss'][fold_idx : fold_idx+num_epochs]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), best_train_loss, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), best_test_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average F1 scores across different folds\n",
    "avg_f1_scores = np.mean(f1_scores_per_fold, axis=0)\n",
    "\n",
    "# Find the index of the best threshold that maximizes the average F1 score\n",
    "best_threshold_avg_index = np.argmax(avg_f1_scores)\n",
    "\n",
    "# Retrieve the actual threshold value corresponding to the best average F1 score\n",
    "best_threshold_avg = thresholds[best_threshold_avg_index]\n",
    "\n",
    "print(\"Avg Threshold:\", best_threshold_avg)\n",
    "print(\"Avg F1 Score:\", np.max(avg_f1_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average training and test loss at specific epochs\n",
    "avg_train_loss = []\n",
    "avg_test_loss = []\n",
    "for epoch in range(num_epochs):\n",
    "    avg_train_loss_at_epoch = np.mean(np.array(fold_results['train_loss'])[epoch::num_epochs], axis=0)\n",
    "    avg_test_loss_at_epoch = np.mean(np.array(fold_results['val_loss'])[epoch::num_epochs], axis=0)\n",
    "    avg_train_loss.append(avg_train_loss_at_epoch)\n",
    "    avg_test_loss.append(avg_test_loss_at_epoch)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), avg_train_loss, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), avg_test_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
