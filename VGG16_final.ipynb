{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global packages\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_curve, roc_auc_score, auc\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    "    mean=[0.3322, 0.0275, 0.1132],\n",
    "    std=[0.2215, 0.0965, 0.3152],\n",
    ")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(\"./Dataset_BUSI/Dataset_BUSI_with_GT/train/\", transform=transform)\n",
    "\n",
    "# Get the labels from the dataset\n",
    "labels = np.array(dataset.targets)\n",
    "\n",
    "# Split the dataset into train and test sets while maintaining class proportions\n",
    "train_indices, test_indices = train_test_split(np.arange(len(dataset)), test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "# Create Subset datasets for train and test\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Optimizer\n",
    "Currently the optimizer is an **Adam Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model,  # Function that creates an optimizer for a given model and hyperparameters\n",
    "                    learning_rate_pretrained=0.00039710,  # Learning rate for pretrained layers\n",
    "                    learning_rate_new=0.00083336,  # Learning rate for new layers\n",
    "                    weight_decay=0.0073691,  # Weight decay (L2 regularization) coefficient\n",
    "                    beta1=0.9,  # Exponential decay rate for the first moment estimates (Adam parameter)\n",
    "                    beta2=0.999,  # Exponential decay rate for the second moment estimates (Adam parameter)\n",
    "                    eps=1e-8,  # Small constant to prevent division by zero (Adam parameter)\n",
    "                    amsgrad=False):  # Flag for using the AMSGrad variant of Adam optimizer\n",
    "\n",
    "    # Separate model parameters into two groups: pretrained and new\n",
    "    params_pretrained = []  # Parameters from the pretrained layers\n",
    "    params_new = []         # Parameters from the new layers\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier.6' in name:\n",
    "            params_new.append(param)  # Parameters from the new layers\n",
    "        else:\n",
    "            params_pretrained.append(param)  # Parameters from the pretrained layers\n",
    "\n",
    "    '''Setting Loss Functions and Optimizers'''\n",
    "    # Create an Adam optimizer with different learning rates for the two parameter groups\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': params_pretrained, 'lr': learning_rate_pretrained},  # Pretrained layer parameters with specific learning rate\n",
    "        {'params': params_new, 'lr': learning_rate_new},                # New layer parameters with different learning rate\n",
    "    ], weight_decay=weight_decay, betas=(beta1, beta2), eps=eps, amsgrad=amsgrad)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, device, train_loader, criterion, optimizer):\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_loss, total_samples = 0.0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculation train loss\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average train loss\n",
    "    avg_train_loss = train_loss / total_samples\n",
    "\n",
    "    return avg_train_loss\n",
    "\n",
    "# Evaluation\n",
    "def testing(model, device, test_loader, criterion, last_epoch=False):\n",
    "\n",
    "    # Set model to configured device\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, total_samples = 0.0, 0\n",
    "    all_predicted_labels = []\n",
    "    all_predicted_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels.unsqueeze(1).float())\n",
    "\n",
    "            predicted_probs = torch.sigmoid(outputs)\n",
    "            all_predicted_probs.extend(predicted_probs.cpu().numpy())\n",
    "\n",
    "            predicted_labels = (predicted_probs >= 0.4).float()\n",
    "            all_predicted_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            test_loss += loss.item() * labels.size(0)\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Calculate average val loss\n",
    "    avg_test_loss = test_loss / total_samples\n",
    "\n",
    "    return avg_test_loss, f1_score(all_labels, all_predicted_labels, average='micro'), all_labels, all_predicted_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Setting training parameters'''\n",
    "num_frozen_layers = 5\n",
    "num_epochs = 20 # epochs\n",
    "batch_size = 32 # batch size\n",
    "k=5 # fold number\n",
    "\n",
    "loss_results={'train_loss': [],\n",
    "              'test_loss': []} # dictionary to memorize all the scores\n",
    "roc_auc_scores = []\n",
    "fpr_list = []\n",
    "tpr_list = []\n",
    "\n",
    "skfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42) # sklearn library\n",
    "\n",
    "'''Setting Loss Function'''\n",
    "criterion = nn.BCEWithLogitsLoss() # Binary Cross Entropy Loss with Sigmoid Function (only have to classify 2 things -> 0 or 1): With multiple classes you generally want to use softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subsampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "test_subsampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=train_subsampler,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    sampler=test_subsampler,\n",
    ")\n",
    "\n",
    "# Initiate the VGG16\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "in_features = model.classifier[6].in_features\n",
    "model.classifier[6] = nn.Linear(in_features=4096, out_features=1)\n",
    "\n",
    "# Freezing some layers\n",
    "for layer in model.features[:num_frozen_layers]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Initiating our optimizer\n",
    "optimizer = create_optimizer(model)\n",
    "\n",
    "# Iterative epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Train and Val\n",
    "    avg_train_loss = training(model, device, train_loader, criterion, optimizer)\n",
    "    avg_test_loss, f1, all_labels, all_predicted_probs = testing(model, device, test_loader, criterion, last_epoch=True)\n",
    "\n",
    "\n",
    "    print(\"Epoch:{}/{} AVG Training Loss:{:.6f} AVG Val Loss:{:.6f} F1 Score: {:.6f}\".format(epoch + 1, num_epochs, avg_train_loss, avg_test_loss, f1))\n",
    "\n",
    "    loss_results['train_loss'].append(avg_train_loss)\n",
    "    loss_results['test_loss'].append(avg_test_loss)\n",
    "\n",
    "    roc_auc = roc_auc_score(all_labels, all_predicted_probs)\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_predicted_probs)\n",
    "\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    fpr_list.append(fpr)\n",
    "    tpr_list.append(tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1, num_epochs + 1), loss_results['train_loss'], label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), loss_results['test_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for epoch in range(num_epochs):\n",
    "    plt.plot(fpr_list[epoch], tpr_list[epoch], lw=2, label=f'Epoch {epoch + 1}')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_curves.png')  # Save the plot as an image file"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
